{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34070a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from model.ipynb\n",
      "2.13.0\n",
      "importing Jupyter notebook from metrics.ipynb\n",
      "Train: 44 - 44 - 44\n",
      "Valid: 6 - 6 - 6\n",
      "Test: 5 - 5 - 5\n",
      "Training started...\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 710s 32s/step - loss: 0.6089 - dice_coef: 0.3911 - iou: 0.2439 - recall: 0.5308 - precision: 0.4323 - val_loss: 0.7022 - val_dice_coef: 0.2978 - val_iou: 0.1752 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-05\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 711s 32s/step - loss: 0.5012 - dice_coef: 0.4988 - iou: 0.3332 - recall: 0.7366 - precision: 0.5243 - val_loss: 0.7083 - val_dice_coef: 0.2917 - val_iou: 0.1709 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 703s 32s/step - loss: 0.4160 - dice_coef: 0.5840 - iou: 0.4138 - recall: 0.8097 - precision: 0.6121 - val_loss: 0.7147 - val_dice_coef: 0.2853 - val_iou: 0.1665 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-05\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 703s 32s/step - loss: 0.3211 - dice_coef: 0.6789 - iou: 0.5152 - recall: 0.8961 - precision: 0.7465 - val_loss: 0.7255 - val_dice_coef: 0.2745 - val_iou: 0.1592 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-05\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 673s 31s/step - loss: 0.2474 - dice_coef: 0.7526 - iou: 0.6045 - recall: 0.9336 - precision: 0.8622 - val_loss: 0.7390 - val_dice_coef: 0.2610 - val_iou: 0.1501 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2011 - dice_coef: 0.7989 - iou: 0.6660 - recall: 0.9477 - precision: 0.9025 \n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "22/22 [==============================] - 674s 31s/step - loss: 0.2011 - dice_coef: 0.7989 - iou: 0.6660 - recall: 0.9477 - precision: 0.9025 - val_loss: 0.7559 - val_dice_coef: 0.2441 - val_iou: 0.1391 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 2710s 128s/step - loss: 0.1817 - dice_coef: 0.8183 - iou: 0.6935 - recall: 0.9511 - precision: 0.9237 - val_loss: 0.7716 - val_dice_coef: 0.2284 - val_iou: 0.1291 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-06\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 768s 35s/step - loss: 0.1772 - dice_coef: 0.8228 - iou: 0.6996 - recall: 0.9537 - precision: 0.9243 - val_loss: 0.7851 - val_dice_coef: 0.2149 - val_iou: 0.1204 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-06\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 739s 34s/step - loss: 0.1751 - dice_coef: 0.8249 - iou: 0.7028 - recall: 0.9552 - precision: 0.9213 - val_loss: 0.7985 - val_dice_coef: 0.2015 - val_iou: 0.1120 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-06\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 10296s 489s/step - loss: 0.1706 - dice_coef: 0.8294 - iou: 0.7094 - recall: 0.9578 - precision: 0.9264 - val_loss: 0.8101 - val_dice_coef: 0.1899 - val_iou: 0.1049 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-06\n",
      "Training completed.\n",
      "INFO:tensorflow:Assets written to: files\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: files\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Image Shape: (512, 512, 3)\n",
      "Sample Mask Shape: (512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from model import build_unet\n",
    "from metrics import dice_loss, dice_coef, iou\n",
    "\n",
    "# Rest of the code...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_dir(path):\n",
    "    \"\"\" Create a directory if it doesn't exist. \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_data(path, split=0.1):\n",
    "    images = sorted(glob(os.path.join(path, \"CXR_png\", \"*.png\")))\n",
    "    masks1 = sorted(glob(os.path.join(path, \"GTMask\", \"leftMask\", \"*.png\")))\n",
    "    masks2 = sorted(glob(os.path.join(path, \"GTMask\", \"rightMask\", \"*.png\")))\n",
    "\n",
    "    train_x, valid_x, train_y1, valid_y1, train_y2, valid_y2 = train_test_split(\n",
    "        images, masks1, masks2, test_size=split, random_state=42\n",
    "    )\n",
    "\n",
    "    train_x, test_x, train_y1, test_y1, train_y2, test_y2 = train_test_split(\n",
    "        train_x, train_y1, train_y2, test_size=split, random_state=42\n",
    "    )\n",
    "\n",
    "    return (train_x, train_y1, train_y2), (valid_x, valid_y1, valid_y2), (test_x, test_y1, test_y2)\n",
    "\n",
    "\n",
    "def read_image(path):\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x.astype(np.float32) / 255.0\n",
    "    return x\n",
    "\n",
    "\n",
    "def read_mask(path1, path2):\n",
    "    x1 = cv2.imread(path1, cv2.IMREAD_GRAYSCALE)\n",
    "    x2 = cv2.imread(path2, cv2.IMREAD_GRAYSCALE)\n",
    "    x = x1 + x2\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x.astype(np.float32) / np.max(x)\n",
    "    x = x > 0.5\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def tf_parse(x, y1, y2):\n",
    "    def _parse(x, y1, y2):\n",
    "        x = x.decode()\n",
    "        y1 = y1.decode()\n",
    "        y2 = y2.decode()\n",
    "\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y1, y2)\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y1, y2], [tf.float32, tf.float32])\n",
    "    x.set_shape([H, W, 3])\n",
    "    y.set_shape([H, W, 1])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def tf_dataset(X, Y1, Y2, batch=8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y1, Y2))\n",
    "    dataset = dataset.shuffle(buffer_size=200)\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    \"\"\" Directory for storing files \"\"\"\n",
    "    create_dir(\"files\")\n",
    "\n",
    "    \"\"\" Global parameters \"\"\"\n",
    "    H = 512\n",
    "    W = 512\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    batch_size = 2\n",
    "    lr = 1e-5\n",
    "    num_epochs = 10\n",
    "    model_path = os.path.join(\"files\", \"model\")\n",
    "    csv_path = os.path.join(\"files\", \"data.csv\")\n",
    "\n",
    "    \"\"\" Dataset \"\"\"\n",
    "    dataset_path = \"C:\\\\Users\\\\IIITSKLM\\\\Desktop\\\\indiana\"\n",
    "\n",
    "    (train_x, train_y1, train_y2), (valid_x, valid_y1, valid_y2), (test_x, test_y1, test_y2) = load_data(dataset_path)\n",
    "    print(f\"Train: {len(train_x)} - {len(train_y1)} - {len(train_y2)}\")\n",
    "    print(f\"Valid: {len(valid_x)} - {len(valid_y1)} - {len(valid_y2)}\")\n",
    "    print(f\"Test: {len(test_x)} - {len(test_y1)} - {len(test_y2)}\")\n",
    "\n",
    "    train_dataset = tf_dataset(train_x, train_y1, train_y2, batch=batch_size)\n",
    "    valid_dataset = tf_dataset(valid_x, valid_y1, valid_y2, batch=batch_size)\n",
    "\n",
    "    \"\"\" Model \"\"\"\n",
    "    model = build_unet((H, W, 3))\n",
    "    metrics = [dice_coef, iou, Recall(), Precision()]\n",
    "    model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=metrics)\n",
    "\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
    "        CSVLogger(csv_path)\n",
    "    ]\n",
    "\n",
    "    print(\"Training started...\")\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=valid_dataset,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    \"\"\" Save the model \"\"\"\n",
    "    model.save(model_path, save_format='tf')\n",
    "\n",
    "    \"\"\" Inspect the data \"\"\"\n",
    "    sample_idx = 0  # Index of the sample to visualize\n",
    "    sample_image = read_image(train_x[sample_idx])\n",
    "    sample_mask = read_mask(train_y1[sample_idx], train_y2[sample_idx])\n",
    "\n",
    "    # Print the shapes of the sample image and mask\n",
    "    print(\"Sample Image Shape:\", sample_image.shape)\n",
    "    print(\"Sample Mask Shape:\", sample_mask.shape)\n",
    "\n",
    "    # Visualize the sample image and mask\n",
    "    cv2.imshow(\"Sample Image\", sample_image)\n",
    "    cv2.imshow(\"Sample Mask\", sample_mask)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f922eda0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660da5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
